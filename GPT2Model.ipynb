{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9222e6b0",
   "metadata": {},
   "source": [
    "# Implementing a GPT model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a498f71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: tensor([[6109, 1110,  318,  534]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize the BPE tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Every day is your\"\n",
    "encoded_input = tokenizer.encode(text, return_tensors='pt')  # Returns a tensor\n",
    "print(f\"Encoded input: {encoded_input}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc9654eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "# Configuration for GPT-2 124M\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a287a",
   "metadata": {},
   "source": [
    "We use short variable names to keep the code compact and maintain readability. Here's a breakdown of key variables used in the model:\n",
    "\n",
    "- **`vocab_size`**: Defines the vocabulary size, which in our case is 50,257 tokens, as determined by the BPE tokenizer.\n",
    "- **`context_length`**: Represents the model's maximum input token length, constrained by the positional embeddings.\n",
    "- **`emb_dim`**: Refers to the embedding size for token inputs, where each input token is converted into a 768-dimensional vector.\n",
    "- **`n_heads`**: Specifies the number of attention heads in the multi-head attention mechanism.\n",
    "- **`n_layers`**: Indicates the number of transformer blocks in the model, which will be implemented in the following sections.\n",
    "- **`drop_rate`**: Determines the intensity of the dropout mechanism, with a value of 0.1 implying that 10% of hidden units are dropped during training to reduce overfitting.\n",
    "- **`qkv_bias`**: Controls whether a bias vector is added in the Linear layers of the multi-head attention mechanism when computing the query (Q), key (K), and value (V) tensors. We disable this feature by default, following common practice in modern large language models. We'll revisit this decision when loading pretrained GPT-2 weights from OpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76abf7",
   "metadata": {},
   "source": [
    "### Let's break down the GPT-2 model into its main components and then implement each block step by step.\n",
    "\n",
    "#### **GPT-2 Model Breakdown:**\n",
    "\n",
    "1. **Token Embedding**\n",
    "2. **Positional Encoding**\n",
    "3. **Transformer Blocks**</br>\n",
    "   -a. Multi-Head Attention  \n",
    "   -b. Layer Normalization  \n",
    "   -c. Feed-Forward Neural Network\n",
    "4. **Output Layer**\n",
    "\n",
    "---\n",
    "\n",
    "### Let's start by implementing each component:\n",
    "\n",
    "#### **1. Token Embedding**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "166e173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding output shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1. Embedding Layer\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        # Initialize the embedding layer with the specified vocabulary size and embedding dimension\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass: convert input token IDs to their corresponding embeddings\n",
    "        return self.embed(x)\n",
    "\n",
    "# Test Embedding\n",
    "# Create an instance of the Embedding layer using the configuration values\n",
    "embedding = Embedding(GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "\n",
    "# Generate random input token IDs with shape (batch_size, seq_length)\n",
    "input_ids = torch.randint(0, GPT_CONFIG_124M[\"vocab_size\"], (2, 10))\n",
    "\n",
    "# Apply the embedding layer to the input token IDs\n",
    "embed_output = embedding(input_ids)\n",
    "\n",
    "# Print the shape of the output embeddings\n",
    "print(f\"Embedding output shape: {embed_output.shape}\")\n",
    "\n",
    "# Assert that the output shape matches the expected shape\n",
    "# Expected shape: (batch_size, seq_length, embed_size)\n",
    "assert embed_output.shape == (2, 10, GPT_CONFIG_124M[\"emb_dim\"]), \"Embedding shape mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9eb35",
   "metadata": {},
   "source": [
    "#### **2. Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1688ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding output shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 2. Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_length=512):\n",
    "        super().__init__()\n",
    "        # Initialize a tensor to hold the positional encodings\n",
    "        pe = torch.zeros(max_seq_length, embed_size)\n",
    "        \n",
    "        # Create a tensor for positions (0 to max_seq_length)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Calculate the division term for the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
    "        \n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Sine for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Cosine for odd indices\n",
    "        \n",
    "        # Register the positional encodings as a buffer (not a model parameter)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: (1, max_seq_length, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positional encodings to the input embeddings\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Test Positional Encoding\n",
    "# Create an instance of the PositionalEncoding layer using the configuration values\n",
    "pos_encoding = PositionalEncoding(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"context_length\"])\n",
    "\n",
    "# Apply the positional encoding to the output of the embedding layer\n",
    "pos_output = pos_encoding(embed_output)\n",
    "\n",
    "# Print the shape of the output after adding positional encodings\n",
    "print(f\"Positional Encoding output shape: {pos_output.shape}\")\n",
    "\n",
    "# Assert that the output shape matches the expected shape\n",
    "assert pos_output.shape == embed_output.shape, \"Positional Encoding shape mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4283a85",
   "metadata": {},
   "source": [
    "#### **3. Transformer Block**\n",
    "\n",
    "##### **A. Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4061fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention output shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# A. Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(embed_size, embed_size, bias=qkv_bias)\n",
    "        self.key = nn.Linear(embed_size, embed_size, bias=qkv_bias)\n",
    "        self.value = nn.Linear(embed_size, embed_size, bias=qkv_bias)\n",
    "        self.out = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        q = self.query(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attention = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attention, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_size)\n",
    "        return self.out(out)\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "mha = MultiHeadAttention(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"n_heads\"])\n",
    "mha_output = mha(pos_output)\n",
    "print(f\"Multi-Head Attention output shape: {mha_output.shape}\")\n",
    "assert mha_output.shape == pos_output.shape, \"Multi-Head Attention shape mismatch\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59783e23",
   "metadata": {},
   "source": [
    "##### **B. LayerNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0070d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Layer Normalization (Just for explanation, we used nn.LayerNorm later)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate mean and variance\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize the input\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d1a6bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm output shape: torch.Size([2, 10, 768])\n",
      "LayerNorm test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test LayerNorm\n",
    "emb_dim = GPT_CONFIG_124M[\"emb_dim\"]\n",
    "layer_norm = LayerNorm(emb_dim)\n",
    "\n",
    "# Create a random input tensor with shape (batch_size, seq_length, emb_dim)\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "input_tensor = torch.randn(batch_size, seq_length, emb_dim)\n",
    "\n",
    "# Apply LayerNorm\n",
    "ln_output = layer_norm(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "print(f\"LayerNorm output shape: {ln_output.shape}\")\n",
    "assert ln_output.shape == input_tensor.shape, \"LayerNorm output shape mismatch\"\n",
    "\n",
    "# Check if the mean and variance of the output are approximately 0 and 1\n",
    "output_mean = ln_output.mean(dim=-1)\n",
    "output_var = ln_output.var(dim=-1, unbiased=False)\n",
    "\n",
    "# Check mean\n",
    "assert torch.allclose(output_mean, torch.zeros(batch_size, seq_length), atol=1e-6), \"Mean is not close to 0\"\n",
    "\n",
    "# Check variance\n",
    "assert torch.allclose(output_var, torch.ones(batch_size, seq_length), atol=1e-6), \"Variance is not close to 1\"\n",
    "\n",
    "print(\"LayerNorm test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c947e9",
   "metadata": {},
   "source": [
    "##### **C. Feed-Forward Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2072458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed-Forward output shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 5. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        # First linear layer that transforms input from embedding size to hidden size\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n",
    "        # Second linear layer that transforms from hidden size back to embedding size\n",
    "        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n",
    "        # GELU activation function\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: apply the first linear layer, then GELU activation, and finally the second linear layer\n",
    "        return self.fc2(self.gelu(self.fc1(x)))\n",
    "\n",
    "# Test Feed-Forward Network\n",
    "# Define the hidden size for the feed-forward network (4 times the embedding size)\n",
    "ff_hidden_size = GPT_CONFIG_124M[\"emb_dim\"] * 4\n",
    "# Create an instance of the FeedForward network\n",
    "ff = FeedForward(GPT_CONFIG_124M[\"emb_dim\"], ff_hidden_size)\n",
    "\n",
    "# Apply the FeedForward network to the output of the multi-head attention layer\n",
    "ff_output = ff(mha_output)\n",
    "\n",
    "# Print the shape of the output after applying the FeedForward network\n",
    "print(f\"Feed-Forward output shape: {ff_output.shape}\")\n",
    "\n",
    "# Assert that the output shape matches the expected shape\n",
    "assert ff_output.shape == mha_output.shape, \"Feed-Forward shape mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e92118",
   "metadata": {},
   "source": [
    "##### **A,B & C combined into Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dbe52fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Block output shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 6. Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_hidden_size, dropout=0.1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # Initialize the multi-head attention layer\n",
    "        self.mha = MultiHeadAttention(embed_size, num_heads, qkv_bias)\n",
    "        # Initialize the feed-forward network\n",
    "        self.ff = FeedForward(embed_size, ff_hidden_size)\n",
    "        # Initialize layer normalization for the attention output\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        # Initialize layer normalization for the feed-forward output\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "        # Initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Apply multi-head attention and add the residual connection, followed by layer normalization\n",
    "        attention_output = self.ln1(x + self.dropout(self.mha(x, mask)))\n",
    "        # Apply feed-forward network and add the residual connection, followed by layer normalization\n",
    "        ff_output = self.ln2(attention_output + self.dropout(self.ff(attention_output)))\n",
    "        return ff_output\n",
    "\n",
    "# Test Transformer Block\n",
    "# Create an instance of the TransformerBlock using the configuration values\n",
    "transformer = TransformerBlock(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"n_heads\"], ff_hidden_size)\n",
    "\n",
    "# Apply the transformer block to the output of the positional encoding layer\n",
    "transformer_output = transformer(pos_output)\n",
    "\n",
    "# Print the shape of the output after applying the transformer block\n",
    "print(f\"Transformer Block output shape: {transformer_output.shape}\")\n",
    "\n",
    "# Assert that the output shape matches the expected shape\n",
    "assert transformer_output.shape == pos_output.shape, \"Transformer Block shape mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa40815",
   "metadata": {},
   "source": [
    "##### **4. GPT-2 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "137201d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Model output shape: torch.Size([2, 64, 50257])\n"
     ]
    }
   ],
   "source": [
    "# 7. GPT-2 Model\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Initialize the embedding layer to convert token IDs to embeddings\n",
    "        self.embedding = Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        \n",
    "        # Initialize positional encoding to add positional information to embeddings\n",
    "        self.positional_encoding = PositionalEncoding(config[\"emb_dim\"], config[\"context_length\"])\n",
    "        \n",
    "        # Create a list of transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            # Each transformer block consists of multi-head attention and feed-forward layers\n",
    "            TransformerBlock(config[\"emb_dim\"], config[\"n_heads\"], config[\"emb_dim\"] * 4, config[\"drop_rate\"], config[\"qkv_bias\"])\n",
    "            for _ in range(config[\"n_layers\"])  # Repeat for the number of layers specified in the config\n",
    "        ])\n",
    "        \n",
    "        # Final linear layer to project the output back to the vocabulary size for logits\n",
    "        self.fc_out = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"])\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Step 1: Convert input token IDs to embeddings and add positional encodings\n",
    "        x = self.dropout(self.positional_encoding(self.embedding(x)))\n",
    "        \n",
    "        # Step 2: Pass the embeddings through each transformer block\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)  # Apply the transformer block with optional masking\n",
    "        \n",
    "        # Step 3: Project the final output to the vocabulary size\n",
    "        return self.fc_out(x)  # Shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "# Test GPT-2 Model\n",
    "# Create an instance of the GPT-2 model using the configuration values\n",
    "model = GPT2(GPT_CONFIG_124M)\n",
    "\n",
    "# Generate random input token IDs with shape (batch_size, seq_length)\n",
    "input_ids = torch.randint(0, GPT_CONFIG_124M[\"vocab_size\"], (2, 64))\n",
    "\n",
    "# Apply the model to the input token IDs\n",
    "output = model(input_ids)\n",
    "\n",
    "# Print the shape of the output from the model\n",
    "print(f\"GPT-2 Model output shape: {output.shape}\")\n",
    "\n",
    "# Assert that the output shape matches the expected shape\n",
    "assert output.shape == (2, 64, GPT_CONFIG_124M[\"vocab_size\"]), \"GPT-2 Model shape mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b65806",
   "metadata": {},
   "source": [
    "## Generate output using GPT-2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95607c23",
   "metadata": {},
   "source": [
    "The modle is not trained yet, but lets try to generate 5 new tokens for our text using GPT-2 untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "811156f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00084b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [6109, 1110, 318, 534]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every day is your\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0037a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[ 6109,  1110,   318,   534, 43407, 42861, 32666, 17714, 43560]])\n",
      "Output length: 9\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=5, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdc5aebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every day is yourriterlatest Basinarin harb\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eac11f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [6109, 1110, 318, 534]\n",
      "Encoded tensor shape: torch.Size([1, 4])\n",
      "Output: tensor([[ 6109,  1110,   318,   534, 43407, 42861, 32666, 17714, 43560]])\n",
      "Output length: 9\n",
      "Decoded text: Every day is yourriterlatest Basinarin harb\n"
     ]
    }
   ],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"\n",
    "    Generate text using the provided model by predicting the next tokens based on the current context.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The language model used for generating text.\n",
    "    - idx: A tensor of shape (batch, n_tokens) containing the current context token indices.\n",
    "    - max_new_tokens: The maximum number of new tokens to generate.\n",
    "    - context_size: The maximum number of tokens to consider from the context.\n",
    "\n",
    "    Returns:\n",
    "    - idx: The updated tensor containing the original context and the newly generated tokens.\n",
    "    \"\"\"\n",
    "    # Loop to generate the specified number of new tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Step 1: Prepare the context\n",
    "        # Crop the current context to the last 'context_size' tokens\n",
    "        idx_cond = idx[:, -context_size:]  # Shape: (batch, context_size)\n",
    "\n",
    "        # Step 2: Get model predictions\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            logits = model(idx_cond)  # Shape: (batch, n_tokens, vocab_size)\n",
    "\n",
    "        # Step 3: Focus on the last time step's predictions\n",
    "        logits = logits[:, -1, :]  # Shape: (batch, vocab_size)\n",
    "\n",
    "        # Step 4: Convert logits to probabilities using softmax\n",
    "        probas = torch.softmax(logits, dim=-1)  # Shape: (batch, vocab_size)\n",
    "\n",
    "        # Step 5: Get the index of the token with the highest probability\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # Shape: (batch, 1)\n",
    "\n",
    "        # Step 6: Append the predicted token index to the sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # Shape: (batch, n_tokens + 1)\n",
    "\n",
    "    return idx  # Return the updated sequence of token indices\n",
    "\n",
    "# Initial context for text generation\n",
    "start_context = \"Every day is your\"\n",
    "\n",
    "# Step 1: Encode the initial context into token indices\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# Step 2: Convert the encoded list into a tensor and add a batch dimension\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Shape: (1, n_tokens)\n",
    "print(\"Encoded tensor shape:\", encoded_tensor.shape)\n",
    "\n",
    "# Set the model to evaluation mode to disable dropout\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Generate new tokens based on the initial context\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=5, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "# Step 4: Print the output tensor and its length\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "# Step 5: Decode the generated token indices back into text\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd6128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
